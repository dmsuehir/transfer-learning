# Copyright (c) 2024 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0


metadata:
  name: llama2-gaudi-multicard
  namespace: kubeflow

secret:
  encodedToken:

image:
  name:  # Specify the image name that was pushed to docker hub or copied to the nodes
  tag:  # Specify the image tag that was pushed to docker hub or copied to the nodes
  pullPolicy: Always

securityContext:
  runAsUser:
  runAsGroup:
  fsGroup:
  privileged: true
  allowPrivilegeEscalation: false

distributed:
  script: /workspace/scripts/finetune.py
  modelNameOrPath: meta-llama/Llama-2-7b-hf
  logLevel: info

  doTrain: True
  doEval: True

  train:
    useMpi: false
    useDeepSpeed: false
    datasetName: medalpaca/medical_meadow_medical_flashcards  # Name of the Hugging Face dataset to use. Leave blank if using a data file
    dataFile:
    datasetConcatenation: True
    promptWithInput: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
    promptWithoutInput: Below is an instruction that describes a task. Write a response that appropriately completes the request.
    perDeviceBatchSize: 12
    epochs: 3
    maxSteps: -1
    maxGradNorm: 0.3
    gradientAccumulationSteps: 1
    learningRate: 1e-4
    lrSchedularType: "constant"
    useFastTokenizer: False
    outputDir: /tmp/pvc-mount/output/saved_model
    loggingSteps: 1020
    saveTotalLimit: 2
    evaluationStrategy: "no"
    saveStrategy: "no"
    warmupRatio: 0.03
    throughputWarmupSteps: 3
    useLora: True
    useLazyMode: True
    loraRank: 8
    loraAlpha: 16
    loraDropout: 0.05
    loraTargetModules: q_proj vproj
    noCuda: True
    overwriteOutputDir: True
    adamEpsilon: 1e-08
    bf16: True
    useHabana: true
  eval:
    perDeviceBatchSize: 8
    validationSplitPercentage: 0.20

envVars:
  ldPreload:
  logLevel: INFO
  transformersCache: /tmp/pvc-mount/transformers_cache
  hfDatasetsCache: /tmp/pvc-mount/hf_dataset_cache
  hfHome: /tmp/home
  httpProxy:
  httpsProxy:
  noProxy:
  ftpProxy:
  socksProxy:

# Resources allocated to each worker
resources:
  hpu: 8
  memoryRequest: 409Gi
  memoryLimit: 409Gi
  nodeSelectorLabel:
  nodeSelectorValue:
  hugePages2Mi: 35202Mi

# Persistent volume claim storage resources
storage:
  storageClassName: nfs-client
  resources: 50Gi
  pvcMountPath: /tmp/pvc-mount
