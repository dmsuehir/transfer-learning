# Copyright (c) 2024 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0


{{- if .Values.distributed.train.useHabana}}
apiVersion: "batch/v1"
kind: Job
metadata:
  name: {{ .Values.metadata.name }}-gaudijob
  namespace: {{ .Values.metadata.namespace }}
spec:
  template:
    spec:
      securityContext:
        runAsUser: {{ .Values.securityContext.runAsUser }}
        runAsGroup: {{ .Values.securityContext.runAsGroup }}
        fsGroup: {{ .Values.securityContext.fsGroup }}
      containers:
        - name: pytorch
          image: {{ .Values.image.name }}:{{ .Values.image.tag }}
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          command:
            - python
            {{- if gt (int .Values.resources.hpu) 1 }}
            - /workspace/optimum-habana/examples/gaudi_spawn.py
            - --world_size
            - "{{ .Values.resources.hpu }}"
            {{- if .Values.distributed.train.useMpi }}
            - --use_mpi
            {{- end }}
            {{- if .Values.distributed.train.useDeepSpeed }}
            - --use_deepspeed
            {{- end }}
            {{- end }}
            - {{ .Values.distributed.script }}
            - --model_name_or_path
            - "{{ .Values.distributed.modelNameOrPath }}"
            {{- if .Values.distributed.train.datasetName }}
            - --dataset_name
            - "{{ .Values.distributed.train.datasetName }}"
            {{- end }}
            - --dataset_cache_directory
            - "{{ .Values.envVars.hfDatasetsCache }}"
            {{- if .Values.distributed.train.dataFile }}
            - --train_file
            - "{{ .Values.distributed.train.dataFile }}"
            {{- end }}
            - --dataset_concatenation
            - "{{ .Values.distributed.train.datasetConcatenation }}"
            - --evaluation_strategy
            - "{{ .Values.distributed.train.evaluationStrategy }}"
            - --prompt_with_input
            - "{{ .Values.distributed.train.promptWithInput }}"
            - --prompt_without_input
            - "{{ .Values.distributed.train.promptWithoutInput }}"
            - --per_device_train_batch_size
            - "{{ .Values.distributed.train.perDeviceBatchSize }}"
            - --per_device_eval_batch_size
            - "{{ .Values.distributed.eval.perDeviceBatchSize }}"
            - --gradient_accumulation_steps
            - "{{ .Values.distributed.train.gradientAccumulationSteps }}"
            - --learning_rate
            - "{{ .Values.distributed.train.learningRate }}"
            - --lr_scheduler_type
            - "{{ .Values.distributed.train.lrSchedularType }}"
            - --num_train_epochs
            - "{{ .Values.distributed.train.epochs }}"
            - --max_steps
            - "{{ .Values.distributed.train.maxSteps }}"
            - --max_grad_norm
            - "{{ .Values.distributed.train.maxGradNorm }}"
            - --logging_steps
            - "{{ .Values.distributed.train.loggingSteps }}"
            - --save_total_limit
            - "{{ .Values.distributed.train.saveTotalLimit }}"
            - --output_dir
            - "{{ .Values.distributed.train.outputDir }}"
            - --validation_split_percentage
            - "{{ .Values.distributed.eval.validationSplitPercentage }}"
            - --log_level
            - "{{ .Values.distributed.logLevel }}"
            - --save_strategy
            - "{{ .Values.distributed.train.saveStrategy }}"
            - --warmup_ratio
            - "{{ .Values.distributed.train.warmupRatio }}"
            - --use_fast_tokenizer
            - "{{ .Values.distributed.train.useFastTokenizer }}"
            - --use_lora
            - "{{ .Values.distributed.train.useLora }}"
            - --lora_rank
            - "{{ .Values.distributed.train.loraRank }}"
            - --lora_alpha
            - "{{ .Values.distributed.train.loraAlpha }}"
            - --lora_dropout
            - "{{ .Values.distributed.train.loraDropout }}"
            - --lora_target_modules
            - "{{ .Values.distributed.train.loraTargetModules }}"
            - --no_cuda
            - "{{ .Values.distributed.train.noCuda }}"
            - --overwrite_output_dir
            - "{{ .Values.distributed.train.overwriteOutputDir }}"
            - --do_train
            - "{{ .Values.distributed.doTrain }}"
            - --do_eval
            - "{{ .Values.distributed.doEval }}"
            - --bf16
            - "{{ .Values.distributed.train.bf16 }}"
            - --adam_epsilon
            - "{{ .Values.distributed.train.adamEpsilon }}"
            - --use_ipex
            - "False"
            - --use_habana
            - "{{ .Values.distributed.train.useHabana }}"
            - --use_lazy_mode
            - "{{ .Values.distributed.train.useLazyMode }}"
            - --throughput_warmup_steps
            - "{{ .Values.distributed.train.throughputWarmupSteps }}"
          env:
          {{- if .Values.envVars.ldPreload }}
          - name: LD_PRELOAD
            value: "{{ .Values.envVars.ldPreload }}"
          {{- end }}
          {{- if .Values.envVars.httpProxy }}
          - name: http_proxy
            value: "{{ .Values.envVars.httpProxy }}"
          {{- end }}
          {{- if .Values.envVars.httpsProxy }}
          - name: https_proxy
            value: "{{ .Values.envVars.httpsProxy }}"
          {{- end }}
          {{- if .Values.envVars.noProxy }}
          - name: no_proxy
            value: "{{ .Values.envVars.noProxy }}"
          {{- end }}
          {{- if .Values.envVars.ftpProxy }}
          - name: ftp_proxy
            value: "{{ .Values.envVars.ftpProxy }}"
          {{- end }}
          {{- if .Values.envVars.socksProxy }}
          - name: socks_proxy
            value: "{{ .Values.envVars.socksProxy }}"
          {{- end }}
          {{- if .Values.envVars.transformersCache }}
          - name: TRANSFORMERS_CACHE
            value: "{{ .Values.envVars.transformersCache }}"
          {{- end }}
          {{- if .Values.envVars.hfDatasetsCache }}
          - name: HF_DATASETS_CACHE
            value: "{{ .Values.envVars.hfDatasetsCache }}"
          {{- end }}
          {{- if .Values.envVars.hfHome }}
          - name: HF_HOME
            value: "{{ .Values.envVars.hfHome }}"
          {{- end }}
          {{- if .Values.envVars.logLevel }}
          - name: LOGLEVEL
            value: "{{ .Values.envVars.logLevel }}"
          {{- end }}
          resources:
            {{- if or .Values.resources.hpu .Values.resources.memoryLimit .Values.resources.hugePages2Mi }}
            limits:
              {{- if .Values.resources.hpu }}
              habana.ai/gaudi: {{ .Values.resources.hpu }}
              {{- end }}
              {{- if .Values.resources.memoryLimit }}
              memory: {{ .Values.resources.memoryLimit }}
              {{- end }}
              {{- if .Values.resources.hugePages2Mi }}
              hugepages-2Mi: {{ .Values.resources.hugePages2Mi }}
              {{- end }}
            {{- end }}
            {{- if or .Values.resources.hpu .Values.resources.memoryLimit .Values.resources.hugePages2Mi }}
            requests:
              {{- if .Values.resources.hpu }}
              habana.ai/gaudi: {{ .Values.resources.hpu }}
              {{- end }}
              {{- if .Values.resources.memoryRequest }}
              memory: {{ .Values.resources.memoryRequest }}
              {{- end }}
              {{- if .Values.resources.hugePages2Mi }}
              hugepages-2Mi: {{ .Values.resources.hugePages2Mi }}
              {{- end }}
            {{- end }}
          volumeMounts:
          - name: output-dir
            mountPath: {{ .Values.storage.pvcMountPath }}
          - mountPath: /dev/shm
            name: dshm
          {{- if .Values.secret.encodedToken}}
          - name: secret-volume
            mountPath: {{ .Values.envVars.hfHome }}
            readOnly: true
          {{- end }}
          {{- if .Values.securityContext.allowPrivilegeEscalation }}
          securityContext:
            allowPrivilegeEscalation: {{ .Values.securityContext.allowPrivilegeEscalation }}
          {{- end }}
          {{- if .Values.securityContext.privileged }}
          securityContext:
            privileged: {{ .Values.securityContext.privileged }}
          {{- end }}
      restartPolicy: Never
      {{- if .Values.resources.nodeSelectorLabel }}
      nodeSelector:
        {{ .Values.resources.nodeSelectorLabel }}: {{ .Values.resources.nodeSelectorValue }}
      {{- end }}
      volumes:
      - name: output-dir
        persistentVolumeClaim:
          claimName: {{ .Values.metadata.name }}-pvc
      - name: dshm
        emptyDir:
          medium: Memory
      {{- if .Values.secret.encodedToken}}
      - name: secret-volume
        secret:
          secretName: {{ .Values.metadata.name }}-secret
      {{- end }}
{{- end }}
