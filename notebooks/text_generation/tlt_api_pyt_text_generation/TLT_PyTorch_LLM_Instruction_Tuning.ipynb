{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f720dcc9",
   "metadata": {},
   "source": [
    "# Instruction Tuning LLMs for Text Generation using PyTorch, Hugging Face, and the IntelÂ® Transfer Learning Tool API\n",
    "\n",
    "This notebook uses the `tlt` library to fine tune a pretrained large language model (LLM) from [Hugging Face](https://huggingface.co) using a custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32c377",
   "metadata": {},
   "source": [
    "## 1. Import dependencies and setup parameters\n",
    "\n",
    "This notebook assumes that you have already followed the instructions to setup a Pytorch environment with all the dependencies required to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdba859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from tlt.datasets import dataset_factory\n",
    "from tlt.models import model_factory\n",
    "from downloader.datasets import DataDownloader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Specify a directory for the dataset to be downloaded\n",
    "dataset_dir = os.environ[\"DATASET_DIR\"] if \"DATASET_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"dataset\")\n",
    "\n",
    "# Specify a directory for output\n",
    "output_dir = os.environ[\"OUTPUT_DIR\"] if \"OUTPUT_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"output\")\n",
    "\n",
    "print(\"Dataset directory:\", dataset_dir)\n",
    "print(\"Output directory:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c6f48",
   "metadata": {},
   "source": [
    "## 2. Get the model\n",
    "\n",
    "In this step, we call the Intel Transfer Learning Tool model factory to list supported Hugging Face text generation models. This is a list of pretrained models from Hugging Face that we tested with our API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See a list of available text generation models\n",
    "model_factory.print_supported_models(use_case='text_generation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578c125",
   "metadata": {},
   "source": [
    "Use the Intel Transfer Learning Tool model factory to get one of the models listed in the previous cell. The `get_model` function returns a TLT model object that will later be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-j-6b\"\n",
    "framework = \"pytorch\"\n",
    "\n",
    "model = model_factory.get_model(model_name, framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45144538",
   "metadata": {},
   "source": [
    "## 3. Load a custom dataset\n",
    "\n",
    "In this example, we download an instruction text dataset example, where each record of the dataset contains text fields for \"instruction\", \"input\", and \"output\" like the following:\n",
    "```\n",
    "{\n",
    "    \"instruction\": \"Convert this sentence into a question.\",\n",
    "    \"input\": \"He read the book.\",\n",
    "    \"output\": \"Did he read the book?\"\n",
    "}\n",
    "```\n",
    "If you are using a custom dataset or downloaded dataset that has similarly formatted json, you can use the same code as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the variables below to use a different json file on your local system.\n",
    "dataset_url = \"https://raw.githubusercontent.com/sahil280114/codealpaca/master/data/code_alpaca_2k.json\"\n",
    "file_name = \"code_alpaca_2k.json\"\n",
    "\n",
    "# If we don't already have the json file, download it\n",
    "if not os.path.exists(os.path.join(dataset_dir, file_name)):\n",
    "    data_downloader = DataDownloader('code_alpaca_2k', dataset_dir, url=dataset_url)\n",
    "    data_downloader.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_factory.load_dataset(dataset_dir=dataset_dir, use_case=\"text_generation\",\n",
    "                                       framework=\"pytorch\", dataset_file=file_name)\n",
    "\n",
    "print(dataset.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38758ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this dictionary for the keys used in your dataset\n",
    "dataset_schema = {\n",
    "    \"instruction_key\": \"instruction\", \n",
    "    \"context_key\": \"input\",\n",
    "    \"response_key\": \"output\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b741459",
   "metadata": {},
   "source": [
    "### Map and tokenize the dataset\n",
    "\n",
    "After describing the schema of your dataset, create formatted prompts out of each example for instruction-tuning. Then preprocess to tokenize the prompts and concatenate them together into longer sequences to speed up fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dict = {\n",
    "    \"prompt_with_context\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{{{instruction_key}}}\\n\\n### Context:\\n{{{context_key}}}\\n\\n### Response:\\n{{{response_key}}}\".format(\n",
    "        **dataset_schema)\n",
    "    ),\n",
    "    \"prompt_without_context\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{{{instruction_key}}}\\n\\n### Response:\\n{{{response_key}}}\".format(**dataset_schema)\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704af2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "dataset.preprocess(model.hub_name, batch_size=32, prompt_dict=prompt_dict, dataset_schema=dataset_schema,\n",
    "                   concatenate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11234b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the dataset and create splits for training and validation\n",
    "dataset.shuffle_split(train_pct=0.75, val_pct=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a2709",
   "metadata": {},
   "source": [
    "## 4. Preview a text completion from the pretrained model\n",
    "\n",
    "Use the generate API to look at some output for a sample prompt. Use this sample prompt or write your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2288a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For code generation custom dataset\n",
    "prompt_template = prompt_dict[\"prompt_with_context\"]\n",
    "test_example = {dataset_schema['instruction_key']: 'Write a Python function that sorts the following list.',\n",
    "               dataset_schema['context_key']: '[3, 2, 1]',\n",
    "               dataset_schema['response_key']: ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0277be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = prompt_template.format_map(test_example)\n",
    "test_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2507f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f1925",
   "metadata": {},
   "source": [
    "## 5. Transfer Learning (Instruction Tuning)\n",
    "\n",
    "The Intel Transfer Learning Tool model's train function is called with the dataset that was just prepared, along with an output directory and the number of training epochs. The model's evaluate function returns a list of metrics calculated from the dataset's validation subset.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "#### Required\n",
    "-  **dataset** (TextGenerationDataset, required): Dataset to use when training the model\n",
    "-  **output_dir** (str): Path to a writeable directory for checkpoint files\n",
    "-  **epochs** (int): Number of epochs to train the model (default: 1)\n",
    "\n",
    "#### Optional\n",
    "-  **initial_checkpoints** (str): Path to checkpoint weights to load. If the path provided is a directory, the latest checkpoint will be used.\n",
    "-  **lora_rank** (int): LoRA rank parameter (default: 8)\n",
    "-  **lora_alpha** (int): LoRA alpha parameter (default: 32)\n",
    "-  **lora_dropout** (float): LoRA dropout parameter (default: 0.05)\n",
    "-  **enable_auto_mixed_precision** (bool or None): Enable auto mixed precision for training. Mixed precision\n",
    "uses both 16-bit and 32-bit floating point types to make training run faster and use less memory. It is recommended to enable auto mixed precision training when running on platforms that support bfloat16 (Intel third or fourth generation Xeon processors). If it is enabled on a platform that does not support bfloat16, it can be detrimental to the training performance. If enable_auto_mixed_precision is set to None, auto mixed precision will be automatically enabled when running with Intel fourth generation Xeon processors, and disabled for other platforms. Defaults to None.\n",
    "\n",
    "Note: refer to release documentation for an up-to-date list of train arguments and their current descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(dataset, output_dir, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a32718",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eeffac",
   "metadata": {},
   "source": [
    "## 6. Export the saved model\n",
    "We can call the model export function to generate a saved model in the Hugging Face format. Each time the model is exported, a new numbered directory is created, which allows identification of the latest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to the output_dir\n",
    "model.export(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b079f",
   "metadata": {},
   "source": [
    "## 7. View the text completion from the fine-tuned model\n",
    "\n",
    "Generate with the test prompt to see if the fine-tuned model gives a better response. You may want to train for at least 3 epochs to see improvement.\n",
    "\n",
    "### Optional Parameters\n",
    "-  **temperature** (float): The value used to modulate the next token probabilities (default: 1.0)\n",
    "-  **top_p** (float): If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation (default: 0.75)\n",
    "-  **top_k** (int):The number of highest probability vocabulary tokens to keep for top-k-filtering (default: 40)\n",
    "-  **repetition_penalty** (float): The parameter for repetition penalty. 1.0 means no penalty. (default: 1.0)\n",
    "-  **num_beams** (int): Number of beams for beam search. 1 means no beam search. (default: 4)\n",
    "-  **max_new_tokens** (int): The maximum number of new tokens generated (default: 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a42b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(test_prompt, repetition_penalty=6.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
